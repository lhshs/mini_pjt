{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFElectraModel.\n",
      "\n",
      "All the layers of TFElectraModel were initialized from the model checkpoint at kykim/electra-kor-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'AlbertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "# electra-base-kor\n",
    "from transformers import ElectraTokenizerFast, ElectraModel, TFElectraModel\n",
    "tokenizer_electra = ElectraTokenizerFast.from_pretrained(\"kykim/electra-kor-base\")\n",
    "\n",
    "model_electra_pt = ElectraModel.from_pretrained(\"kykim/electra-kor-base\")    # pytorch\n",
    "model_electra_tf = TFElectraModel.from_pretrained(\"kykim/electra-kor-base\")  # tensorflow\n",
    "\n",
    "# bert-base-kor\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"kykim/bert-kor-base\")\n",
    "model_bert = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n",
    "\n",
    "# albert-base-kor\n",
    "from transformers import BertTokenizerFast, AlbertModel\n",
    "tokenizer_albert = BertTokenizerFast.from_pretrained(\"kykim/albert-kor-base\")\n",
    "model_albert = AlbertModel.from_pretrained(\"kykim/albert-kor-base\")\n",
    "\n",
    "# funnel-base-kor\n",
    "from transformers import FunnelTokenizerFast, FunnelModel\n",
    "tokenizer_funnel = FunnelTokenizerFast.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "model_funnel = FunnelModel.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "\n",
    "# gpt3-kor-small_based_on_gpt2\n",
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer_gpt3 = BertTokenizerFast.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
    "model_gpt3 = GPT2LMHeadModel.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
    "\n",
    "# bertshared-kor-base\n",
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "tokenizer_bertshared = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "model_bertshared = EncoderDecoderModel.from_pretrained(\"kykim/bertshared-kor-base\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
