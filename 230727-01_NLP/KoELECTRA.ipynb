{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/monologg/KoELECTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice = pd.read_csv('data/icecream_emot.csv')\n",
    "tsh = pd.read_csv('data/sherpa_emot3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë²•ì •ì—°ìˆ˜ ìœ ìš©í•˜ë„¤ìš” ì˜ë¬´ì ìœ¼ë¡œ í•´ë§ˆë‹¤ ë“£ëŠ” ì—°ìˆ˜ì¸ë° ë‹¤ë¥¸ ê°•ì‚¬ì—ê²Œ ë“£ê³  ì‹¶ì–´ ì‹ ì²­í–ˆ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë…ë¦½ìš´ë™ì— ê´€í•œ ì§€ì‹ ìŒ“ê¸° ë…ë¦½ìš´ë™ì— ëŒ€í•´ì„œ ì•Œê³ , í•™ìƒ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì—…í•˜ê¸° ì¢‹ì•˜ìŠµë‹ˆ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì‚¶ì„ ë°”ê¾¸ëŠ” ì‚¬íšŒìˆ˜ì—… ì£¼ì œí†µí•© ì§ˆë¬¸ê³¼ ìƒìƒìœ¼ë¡œ ì§„í–‰í•˜ê¸° ì‚¬íšŒ êµê³¼ì„œ ê°œì •ë˜ê³  í•™ìŠµëŸ‰...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì˜ì–´ íšŒí™”ê°€ ë§‰ë§‰í•œ ì„ ìƒë‹˜ë“¤ê»˜ ìœ ìµí•œ ì—°ìˆ˜! ì˜ì–´ê°€ ë„ˆë¬´ ì˜¤ëœë§Œì´ì‹œê±°ë‚˜ ì¼ìƒìƒí™œì—ì„œ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ë…ì„œêµìœ¡ì„ í•´ë³´ê³  ì‹¶ì–´ì§€ê²Œ ë§Œë“œëŠ” ì—°ìˆ˜!!! ì•„ì´ë¥¼ í‚¤ìš°ë©° ê·¸ë¦¼ì±…ê³¼ ë™í™”ì±…ì— ê´€ì‹¬ì„...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>ì•Œì°¬ì—°ìˆ˜ ê³ ë§™ìŠµë‹ˆë‹¤.^^ í•™êµ ì§„ë¡œêµìœ¡ì˜ ëª©í‘œ (2015ê°œì •)í•™ìƒ ìì‹ ì˜ ì§„ë¡œë¥¼ ì°½...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>ìŠ¤í† ë¦¬í…”ë§ì„ í™œìš©í•œ ì´ˆë“±ìˆ˜í•™ ìˆ˜ì—… ì—°ìˆ˜ êµ¿êµ¿! ì„ ìƒë‹˜ë“¤ì˜ ìˆ˜í•™ ê°œë… ì„¤ëª…ì„ ë“£ê³ , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>ê³ ë§™ìŠµë‹ˆë‹¤. ì§„ë¡œë¼ëŠ”ê²Œ í¥ë¯¸ëŠ” ìˆì§€ë§Œ ë„ˆë¬´ í¬ê´„ì ì´ë„¤ìš”.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>ê°ì‚¬í•©ë‹ˆë‹¤^^ ì§„ë¡œêµìœ¡ì— ê´€í•´ ë§ì€ ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\\nê°ì‚¬í•©ë‹ˆë‹¤.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>ë„ˆë¬´ ìœ ìµí•œ ì—°ìˆ˜ì˜€ìŠµë‹ˆë‹¤. ìë…€ê°€ í˜„ì¬ ê³ ë“±í•™êµì— ì¬í•™ì¤‘ì´ê³  ë˜í•œ ì˜¤ë˜ì „ë¶€í„° ì§„ë¡œêµ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2600 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score\n",
       "0     ë²•ì •ì—°ìˆ˜ ìœ ìš©í•˜ë„¤ìš” ì˜ë¬´ì ìœ¼ë¡œ í•´ë§ˆë‹¤ ë“£ëŠ” ì—°ìˆ˜ì¸ë° ë‹¤ë¥¸ ê°•ì‚¬ì—ê²Œ ë“£ê³  ì‹¶ì–´ ì‹ ì²­í–ˆ...      1\n",
       "1     ë…ë¦½ìš´ë™ì— ê´€í•œ ì§€ì‹ ìŒ“ê¸° ë…ë¦½ìš´ë™ì— ëŒ€í•´ì„œ ì•Œê³ , í•™ìƒ ëŒ€ìƒìœ¼ë¡œ ìˆ˜ì—…í•˜ê¸° ì¢‹ì•˜ìŠµë‹ˆ...      1\n",
       "2     ì‚¶ì„ ë°”ê¾¸ëŠ” ì‚¬íšŒìˆ˜ì—… ì£¼ì œí†µí•© ì§ˆë¬¸ê³¼ ìƒìƒìœ¼ë¡œ ì§„í–‰í•˜ê¸° ì‚¬íšŒ êµê³¼ì„œ ê°œì •ë˜ê³  í•™ìŠµëŸ‰...      1\n",
       "3     ì˜ì–´ íšŒí™”ê°€ ë§‰ë§‰í•œ ì„ ìƒë‹˜ë“¤ê»˜ ìœ ìµí•œ ì—°ìˆ˜! ì˜ì–´ê°€ ë„ˆë¬´ ì˜¤ëœë§Œì´ì‹œê±°ë‚˜ ì¼ìƒìƒí™œì—ì„œ...      1\n",
       "4     ë…ì„œêµìœ¡ì„ í•´ë³´ê³  ì‹¶ì–´ì§€ê²Œ ë§Œë“œëŠ” ì—°ìˆ˜!!! ì•„ì´ë¥¼ í‚¤ìš°ë©° ê·¸ë¦¼ì±…ê³¼ ë™í™”ì±…ì— ê´€ì‹¬ì„...      1\n",
       "...                                                 ...    ...\n",
       "2595  ì•Œì°¬ì—°ìˆ˜ ê³ ë§™ìŠµë‹ˆë‹¤.^^ í•™êµ ì§„ë¡œêµìœ¡ì˜ ëª©í‘œ (2015ê°œì •)í•™ìƒ ìì‹ ì˜ ì§„ë¡œë¥¼ ì°½...      1\n",
       "2596  ìŠ¤í† ë¦¬í…”ë§ì„ í™œìš©í•œ ì´ˆë“±ìˆ˜í•™ ìˆ˜ì—… ì—°ìˆ˜ êµ¿êµ¿! ì„ ìƒë‹˜ë“¤ì˜ ìˆ˜í•™ ê°œë… ì„¤ëª…ì„ ë“£ê³ , ...      1\n",
       "2597                    ê³ ë§™ìŠµë‹ˆë‹¤. ì§„ë¡œë¼ëŠ”ê²Œ í¥ë¯¸ëŠ” ìˆì§€ë§Œ ë„ˆë¬´ í¬ê´„ì ì´ë„¤ìš”.      1\n",
       "2598           ê°ì‚¬í•©ë‹ˆë‹¤^^ ì§„ë¡œêµìœ¡ì— ê´€í•´ ë§ì€ ê²ƒì„ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\\nê°ì‚¬í•©ë‹ˆë‹¤.      1\n",
       "2599  ë„ˆë¬´ ìœ ìµí•œ ì—°ìˆ˜ì˜€ìŠµë‹ˆë‹¤. ìë…€ê°€ í˜„ì¬ ê³ ë“±í•™êµì— ì¬í•™ì¤‘ì´ê³  ë˜í•œ ì˜¤ë˜ì „ë¶€í„° ì§„ë¡œêµ...      1\n",
       "\n",
       "[2600 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsh = tsh[['title_comments', 'star']]\n",
    "tsh.rename(columns={'title_comments' : 'text', 'star' : 'score'}, inplace=True)\n",
    "tsh['score'] = tsh['score'].apply(lambda x : int(x.replace('ì ','')))\n",
    "tsh['score'] = tsh['score'].apply(lambda x : 0 if x == 3 else (1 if x > 3 else -1))\n",
    "tsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì•Œì°¨ë„¤ìš” ë¬´ì—‡ë³´ë‹¤ë„ ë¹¨ë¦¬ë“¤ì„ ìˆ˜ ìˆì–´ ì¢‹ìŠµë‹ˆë‹¤</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ëª…ë¶ˆí—ˆì „ ëª…ê°•ì‚¬ë¼ ê·¸ëŸ°ì§€ ë§ì”€ì„ ì°¸ ì˜í•˜ì‹œë„¤ìš”~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°•ì¶” 30ë…„ ê°€ê¹Œì´ ì¼í•˜ë©´ì„œë„ ë†“ì¹˜ê³  ìˆì—ˆë˜ ê²ƒë“¤ì„ ì•Œë ¤ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë¬´ì¡°ê±´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>íŒ¨ë“¤ë ›ê³¼ ëµì»¤ë²¨ í™œìš© ì˜ í•  ìˆ˜ ìˆì–´ìš” íŒ¨ë“¤ë ›ê³¼ ëµì»¤ë²¨ í™œìš©í•˜ê³  ìˆì—ˆìœ¼ë‚˜ ë‹¤ì–‘í•œ êµ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>í•„ìˆ˜ ì—°ìˆ˜ ë¶€ë‹´ ì—†ì´ í¸ì•ˆí•˜ê²Œ ì•ˆì „ ì—°ìˆ˜ ì˜¬ í•´ ê¼­ ë°›ì•„ì•¼ í•˜ëŠ”ë°, ë¶€ë‹´ ì—†ì´ í¸ì•ˆ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>ì•Œë² ë¥´í† ê°€ í•¨ê»˜í•˜ëŠ” í•µì‹¬ì—­ëŸ‰ ì—°ê³„ ë‹¤ë¬¸í™” êµìœ¡ ì—¬íƒ€ ì—°ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš©...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>ì „ë¬¸ê°€ê°€ ì•Œë ¤ì£¼ëŠ” ì°½ì˜ ì˜ì¬êµìœ¡ì˜ ë¹„ë²• ì—°ìˆ˜ í›„ê¸° ì˜ì¬ êµìœ¡ì— ëŒ€í•œ í‹€ì„ ì¡ì•„ì£¼ì‹œëŠ”...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>ë„ë„í•œ êµì‚¬ìƒí™œ ì—°ìˆ˜ë¥¼ í†µí•´ ìì‹ ê°ì„ ë” ê°–ê²Œ ë˜ì—ˆë‹¤.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>êµì‹¤ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì•±ë“¤ ì¤‘ì‹¬ìœ¼ë¡œ ì¬ë¯¸ìˆê²Œ ë“¤ì—ˆì–´ìš”. êµì‹¤ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ”...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>ë‚˜ë„ ë”°ë¼í•˜ê²Œ í•˜ëŠ” ìº˜ë¦¬ê·¸ë¼í”¼ ì—°ìˆ˜ ê¸€ì”¨ì™€ ê·¸ë¦¼ì„ í˜ë“¤ì–´í•˜ëŠ” ë‚˜ë„ ë”°ë¼í•˜ê²Œ í•˜ëŠ” ìº˜...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5241 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score\n",
       "0                             ì•Œì°¨ë„¤ìš” ë¬´ì—‡ë³´ë‹¤ë„ ë¹¨ë¦¬ë“¤ì„ ìˆ˜ ìˆì–´ ì¢‹ìŠµë‹ˆë‹¤      1\n",
       "1                            ëª…ë¶ˆí—ˆì „ ëª…ê°•ì‚¬ë¼ ê·¸ëŸ°ì§€ ë§ì”€ì„ ì°¸ ì˜í•˜ì‹œë„¤ìš”~      1\n",
       "2     ê°•ì¶” 30ë…„ ê°€ê¹Œì´ ì¼í•˜ë©´ì„œë„ ë†“ì¹˜ê³  ìˆì—ˆë˜ ê²ƒë“¤ì„ ì•Œë ¤ì£¼ì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë¬´ì¡°ê±´ ...      1\n",
       "3     íŒ¨ë“¤ë ›ê³¼ ëµì»¤ë²¨ í™œìš© ì˜ í•  ìˆ˜ ìˆì–´ìš” íŒ¨ë“¤ë ›ê³¼ ëµì»¤ë²¨ í™œìš©í•˜ê³  ìˆì—ˆìœ¼ë‚˜ ë‹¤ì–‘í•œ êµ...      1\n",
       "4     í•„ìˆ˜ ì—°ìˆ˜ ë¶€ë‹´ ì—†ì´ í¸ì•ˆí•˜ê²Œ ì•ˆì „ ì—°ìˆ˜ ì˜¬ í•´ ê¼­ ë°›ì•„ì•¼ í•˜ëŠ”ë°, ë¶€ë‹´ ì—†ì´ í¸ì•ˆ...      1\n",
       "...                                                 ...    ...\n",
       "5236  ì•Œë² ë¥´í† ê°€ í•¨ê»˜í•˜ëŠ” í•µì‹¬ì—­ëŸ‰ ì—°ê³„ ë‹¤ë¬¸í™” êµìœ¡ ì—¬íƒ€ ì—°ìˆ˜ì™€ ë‹¤ë¥´ê²Œ ìƒˆë¡œ ì•Œê²Œëœ ë‚´ìš©...      1\n",
       "5237  ì „ë¬¸ê°€ê°€ ì•Œë ¤ì£¼ëŠ” ì°½ì˜ ì˜ì¬êµìœ¡ì˜ ë¹„ë²• ì—°ìˆ˜ í›„ê¸° ì˜ì¬ êµìœ¡ì— ëŒ€í•œ í‹€ì„ ì¡ì•„ì£¼ì‹œëŠ”...      1\n",
       "5238                     ë„ë„í•œ êµì‚¬ìƒí™œ ì—°ìˆ˜ë¥¼ í†µí•´ ìì‹ ê°ì„ ë” ê°–ê²Œ ë˜ì—ˆë‹¤.      1\n",
       "5239  êµì‹¤ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì•±ë“¤ ì¤‘ì‹¬ìœ¼ë¡œ ì¬ë¯¸ìˆê²Œ ë“¤ì—ˆì–´ìš”. êµì‹¤ì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ”...      1\n",
       "5240  ë‚˜ë„ ë”°ë¼í•˜ê²Œ í•˜ëŠ” ìº˜ë¦¬ê·¸ë¼í”¼ ì—°ìˆ˜ ê¸€ì”¨ì™€ ê·¸ë¦¼ì„ í˜ë“¤ì–´í•˜ëŠ” ë‚˜ë„ ë”°ë¼í•˜ê²Œ í•˜ëŠ” ìº˜...      1\n",
       "\n",
       "[5241 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice = ice[['title_comments', 'rated']]\n",
    "ice.rename(columns={'title_comments' : 'text', 'rated' : 'score'}, inplace=True)\n",
    "ice['score'] = ice['score'].apply(lambda x : int(x[-1]))\n",
    "ice['score'] = ice['score'].apply(lambda x : 0 if x == 3 else (1 if x > 3 else -1))\n",
    "ice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tsh modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\u001b[95mklue/bert-base\u001b[0m=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\hslio\\AppData\\Local\\Temp\\ipykernel_9540\\1665998401.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ice[pre_model][idx] = '1'\n",
      "9it [00:00, 21.84it/s]C:\\Users\\hslio\\AppData\\Local\\Temp\\ipykernel_9540\\1665998401.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ice[pre_model][idx] = '-1'\n",
      "C:\\Users\\hslio\\AppData\\Local\\Temp\\ipykernel_9540\\1665998401.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ice[pre_model][idx] = '0'\n",
      "37it [00:01, 20.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[233], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m# predict\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m idx, review \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(review_list)):\n\u001b[1;32m---> 34\u001b[0m     pred \u001b[39m=\u001b[39m sentiment_classifier(review)\n\u001b[0;32m     35\u001b[0m     \u001b[39m# print(f'{review}\\n>> {pred[0]}')\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m (pred[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m&\u001b[39m(pred[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.51\u001b[39m) \u001b[39m|\u001b[39m (pred[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLABEL_1\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m&\u001b[39m(pred[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.51\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\pipelines\\base.py:1122\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1115\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1116\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         )\n\u001b[0;32m   1120\u001b[0m     )\n\u001b[0;32m   1121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\pipelines\\base.py:1129\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1128\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1129\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1130\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1131\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\pipelines\\base.py:1028\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1027\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1028\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1029\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1030\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:286\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    277\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    278\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    285\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 286\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery(hidden_states)\n\u001b[0;32m    288\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hslio\\.conda\\envs\\lhs\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_lst = ['jaehyeong/koelectra-base-v3-generalized-sentiment-analysis', \n",
    "            'klue/bert-base', \n",
    "            'matthewburke/korean_sentiment',\n",
    "            # 'monologg/koelectra-base-discriminator',\n",
    "            # \"monologg/koelectra-small-discriminator\",\n",
    "            \"monologg/koelectra-base-v2-discriminator\",\n",
    "            # \"monologg/koelectra-small-v2-discriminator\",\n",
    "            # \"monologg/koelectra-base-v3-discriminator\",\n",
    "            # \"monologg/koelectra-small-v3-discriminator\",\n",
    "            # \"facebook/bart-large-mnli\",\n",
    "            # \"monologg/koelectra-base-v3-goemotions\",\n",
    "            'skt/kogpt2-base-v2',\n",
    "            # 'skt/kobert-base-v1',\n",
    "            'beomi/kcbert-large',\n",
    "            # \"google/electra-small-discriminator\"\n",
    "            ]\n",
    "\n",
    "\n",
    "for i in model_lst[1:]: # 0,1,2,3\n",
    "    # load model\n",
    "    pre_model = i\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pre_model)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pre_model)\n",
    "    sentiment_classifier = TextClassificationPipeline(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    print('====='+ '\\033[95m' + pre_model + '\\033[0m' + '=====')\n",
    "\n",
    "    # target reviews\n",
    "    review_list = list(ice.text)\n",
    "\n",
    "    ice[pre_model] = '-'\n",
    "    # predict\n",
    "    for idx, review in tqdm(enumerate(review_list)):\n",
    "        pred = sentiment_classifier(review)\n",
    "        # print(f'{review}\\n>> {pred[0]}')\n",
    "        if (pred[0]['label']=='1')&(pred[0]['score'] > 0.51) | (pred[0]['label']=='LABEL_1')&(pred[0]['score'] > 0.51):\n",
    "            ice[pre_model][idx] = '1'\n",
    "        elif (pred[0]['label']=='0')&(pred[0]['score']>0.51) | (pred[0]['label']=='LABEL_0')&(pred[0]['score'] > 0.51):  \n",
    "            ice[pre_model][idx] = '-1'\n",
    "        else:\n",
    "            ice[pre_model][idx] = '0'\n",
    "\n",
    "        # print(f'{review}\\n>> {pred[0]}')\n",
    "\n",
    "print('ğŸ¥•')\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score\n",
      " 1       5146\n",
      " 0         64\n",
      "-1         31\n",
      "dtype: int64\n",
      "\n",
      "jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\n",
      "1                                                             5135\n",
      "-1                                                             106\n",
      "dtype: int64\n",
      "\n",
      "matthewburke/korean_sentiment\n",
      "0                                5241\n",
      "dtype: int64\n",
      "\n",
      "klue/bert-base\n",
      "0                 5241\n",
      "dtype: int64\n",
      "\n",
      "monologg/koelectra-base-v2-discriminator\n",
      "0                                           5241\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ice.iloc[:, 1:].columns:\n",
    "    print(ice[[i]].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['score', 'jaehyeong/koelectra-base-v3-generalized-sentiment-analysis',\n",
       "       'matthewburke/korean_sentiment', 'klue/bert-base',\n",
       "       'monologg/koelectra-base-v2-discriminator'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice.iloc[:, 1:].columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
